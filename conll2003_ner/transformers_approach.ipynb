{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b850b-face-40e7-b1da-1ccba210895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install spacy\n",
    "! pip install transformers\n",
    "! pip install datasets\n",
    "!pip show seqeval\n",
    "!pip install -U seqeval\n",
    "! pip install scikit-learn\n",
    "! pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7f8c9-b38d-433d-81de-08c86d09c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431218f7-c59e-46fa-8a09-8bdbd23483e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c8720e-878d-4679-bbe7-ae8b8a3f301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "datasets = load_dataset(\"conll2003\")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model, num_labels=len(datasets[\"train\"].features[\"ner_tags\"].feature.names))\n",
    "\n",
    "\n",
    "# https://huggingface.co/docs/transformers/en/tasks/token_classification\n",
    "# Adjusted tokenization and alignment function\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=False)\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word in the inputs\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)  # Special token or same word as previous token\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
    "\n",
    "# Initialise the data collator with padding\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialise Trainer with the new data collator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model with the adjusted settings\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "label_list = datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "\n",
    "# Tunstall, Lewis; Werra, Leandro von; Wolf, Thomas. Natural Language Processing with Transformers\n",
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    preds_list_str = []\n",
    "    labels_list_str = []\n",
    "\n",
    "    for batch_index in range(len(label_ids)):\n",
    "        batch_preds = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for idx, label_id in enumerate(label_ids[batch_index]):\n",
    "            if label_id != -100:  # Ensure we only consider real labels, not padding\n",
    "                true_label = label_list[label_id]\n",
    "                batch_labels.append(true_label)\n",
    "                \n",
    "                pred_label_id = preds[batch_index][idx]\n",
    "                pred_label = label_list[pred_label_id]\n",
    "                batch_preds.append(pred_label)\n",
    "\n",
    "        preds_list_str.append(batch_preds)\n",
    "        labels_list_str.append(batch_labels)\n",
    "\n",
    "    return preds_list_str, labels_list_str\n",
    "\n",
    "\n",
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "preds_list, labels_list = align_predictions(predictions, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc4a98d-ff9d-4818-9c3a-da871db05168",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_preds = [label for sublist in preds_list for label in sublist]\n",
    "flat_labels = [label for sublist in labels_list for label in sublist]\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(flat_labels + flat_preds)\n",
    "\n",
    "split_index = len(flat_labels)\n",
    "\n",
    "flat_encoded_labels = encoded_labels[:split_index]\n",
    "flat_encoded_preds = encoded_labels[split_index:]\n",
    "\n",
    "# Compute metrics\n",
    "print(classification_report(flat_encoded_labels, flat_encoded_preds, target_names=label_encoder.classes_, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df4260-7343-4d81-91b2-26f94b5315fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "results = metric.compute(predictions=preds_list, references=labels_list, scheme=\"IOB2\", mode=\"strict\")\n",
    "print(\"Full entity evaluation Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b1b2e4-f1d4-41c3-a7de-c06967db9af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(flat_encoded_labels, flat_encoded_preds)\n",
    "\n",
    "custom_order = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "# Get indices for the new order based on label_encoder\n",
    "order_indices = [label_encoder.transform([label])[0] for label in custom_order]\n",
    "new_cm = cm[order_indices, :][:, order_indices]\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(new_cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=custom_order, yticklabels=custom_order)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Transformers Confusion Matrix')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
